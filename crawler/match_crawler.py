# -*- coding: utf-8 -*-
"""
Match Crawler - Crawler chuy√™n d·ª•ng cho l·ªãch thi ƒë·∫•u
"""

import logging
from logging.handlers import RotatingFileHandler
from colorama import init, Fore, Style
from database import DatabaseHandler
from parsers import VnExpressMatchParser, RobongMatchParser
from config import MATCH_SOURCES, LOG_FILE
import time
import sys
from datetime import datetime, timedelta

# Set UTF-8 encoding for Windows console
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Initialize colorama
init(autoreset=True)

# Setup logging
def setup_logging():
    """C·∫•u h√¨nh logging"""
    logger = logging.getLogger('match_crawler')
    logger.setLevel(logging.INFO)
    
    # File handler
    log_file = LOG_FILE.parent / 'match_crawler.log'
    file_handler = RotatingFileHandler(
        log_file, 
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()


class MatchCrawler:
    """Crawler chuy√™n d·ª•ng cho l·ªãch thi ƒë·∫•u"""
    
    def __init__(self):
        self.db = DatabaseHandler()
        self.match_parsers = {
            'VnExpressMatchParser': VnExpressMatchParser(),
            'RobongMatchParser': RobongMatchParser(),
        }
        self.stats = {
            'matches_crawled': 0,
            'matches_saved': 0,
            'matches_skipped': 0,
            'matches_errors': 0
        }
    
    def print_header(self):
        """In header ƒë·∫πp"""
        try:
            print(f"\n{Fore.CYAN}{'='*70}")
            print(f"{Fore.CYAN}|{' '*68}|")
            print(f"{Fore.CYAN}|{Fore.YELLOW}{'MATCH SCHEDULE CRAWLER':^68}{Fore.CYAN}|")
            print(f"{Fore.CYAN}|{Fore.GREEN}{'C√¥ng c·ª• crawl l·ªãch thi ƒë·∫•u t·ª± ƒë·ªông':^68}{Fore.CYAN}|")
            print(f"{Fore.CYAN}|{' '*68}|")
            print(f"{Fore.CYAN}{'='*70}{Style.RESET_ALL}\n")
        except UnicodeEncodeError:
            # Fallback to ASCII if Unicode fails
            print(f"\n{Fore.CYAN}{'='*70}")
            print(f"{Fore.CYAN}|{' '*68}|")
            print(f"{Fore.CYAN}|{Fore.YELLOW}{'MATCH SCHEDULE CRAWLER':^68}{Fore.CYAN}|")
            print(f"{Fore.CYAN}|{Fore.GREEN}{'C√¥ng c·ª• crawl l·ªãch thi ƒë·∫•u t·ª± ƒë·ªông':^68}{Fore.CYAN}|")
            print(f"{Fore.CYAN}|{' '*68}|")
            print(f"{Fore.CYAN}{'='*70}{Style.RESET_ALL}\n")
    
    def print_stats(self):
        """In th·ªëng k√™"""
        try:
            print(f"\n{Fore.YELLOW}{'-'*70}")
            print(f"{Fore.YELLOW}TH·ªêNG K√ä CRAWLER L·ªäCH THI ƒê·∫§U:")
            print(f"{Fore.GREEN}  [OK] T·ªïng s·ªë tr·∫≠n ƒë·∫•u crawl: {self.stats['matches_crawled']}")
            print(f"{Fore.GREEN}  [OK] ƒê√£ l∆∞u th√†nh c√¥ng: {self.stats['matches_saved']}")
            print(f"{Fore.YELLOW}  [SKIP] ƒê√£ b·ªè qua (tr√πng): {self.stats['matches_skipped']}")
            print(f"{Fore.RED}  [ERROR] L·ªói: {self.stats['matches_errors']}")
            print(f"{Fore.YELLOW}{'-'*70}{Style.RESET_ALL}\n")
        except UnicodeEncodeError:
            # Fallback to ASCII
            print(f"\n{Fore.YELLOW}{'-'*70}")
            print(f"{Fore.YELLOW}THONG KE CRAWLER LICH THI DAU:")
            print(f"{Fore.GREEN}  [OK] Tong so tran dau crawl: {self.stats['matches_crawled']}")
            print(f"{Fore.GREEN}  [OK] Da luu thanh cong: {self.stats['matches_saved']}")
            print(f"{Fore.YELLOW}  [SKIP] Da bo qua (trung): {self.stats['matches_skipped']}")
            print(f"{Fore.RED}  [ERROR] Loi: {self.stats['matches_errors']}")
            print(f"{Fore.YELLOW}{'-'*70}{Style.RESET_ALL}\n")
    
    def crawl_matches(self, source_name, source_config, limit=50, days_range=None):
        """
        Crawl c√°c tr·∫≠n ƒë·∫•u s·∫Øp di·ªÖn ra t·ª´ m·ªôt ngu·ªìn
        
        Args:
            source_name: T√™n ngu·ªìn
            source_config: Config c·ªßa ngu·ªìn
            limit: S·ªë l∆∞·ª£ng tr·∫≠n ƒë·∫•u t·ªëi ƒëa
            days_range: Tuple (days_before, days_after) ƒë·ªÉ filter theo ng√†y
                       V√≠ d·ª•: (1, 1) = h√¥m qua, h√¥m nay, h√¥m sau
        """
        if not source_config.get('enabled', False):
            logger.info(f"[SKIP] Ngu·ªìn matches {source_name} ƒë√£ b·ªã t·∫Øt")
            return
        
        print(f"\n{Fore.CYAN}‚ñ∂ B·∫Øt ƒë·∫ßu crawl l·ªãch thi ƒë·∫•u: {source_config['name']}")
        print(f"{Fore.CYAN}  URL: {source_config['base_url']}{Style.RESET_ALL}")
        
        parser_name = source_config.get('parser')
        if parser_name not in self.match_parsers:
            logger.error(f"[ERROR] Kh√¥ng t√¨m th·∫•y match parser: {parser_name}")
            return
        
        parser = self.match_parsers[parser_name]
        
        # C·∫≠p nh·∫≠t base_url t·ª´ config
        if hasattr(parser, 'base_url'):
            parser.base_url = source_config['base_url']
        
        try:
            # L·∫•y danh s√°ch tr·∫≠n ƒë·∫•u v·ªõi filter theo ng√†y
            matches = parser.get_upcoming_matches(limit=limit, days_range=days_range)
            
            if not matches:
                logger.warning(f"[WARN] Kh√¥ng t√¨m th·∫•y tr·∫≠n ƒë·∫•u n√†o t·ª´ {source_name}")
                return
            
            print(f"{Fore.GREEN}  [OK] T√¨m th·∫•y {len(matches)} tr·∫≠n ƒë·∫•u\n")
            
            # L∆∞u t·ª´ng tr·∫≠n ƒë·∫•u
            for idx, match_info in enumerate(matches, 1):
                home_team_name = match_info.get('home_team_name', 'Unknown')
                away_team_name = match_info.get('away_team_name', 'Unknown')
                match_date = match_info.get('match_date', datetime.now())
                
                print(f"{Fore.CYAN}  [{idx}/{len(matches)}] {home_team_name} vs {away_team_name}")
                print(f"      Ng√†y: {match_date.strftime('%d/%m/%Y %H:%M') if isinstance(match_date, datetime) else match_date}")
                
                self.stats['matches_crawled'] += 1
                
                # L·∫•y ho·∫∑c t·∫°o teams
                home_team_id = self.db.get_or_create_team(
                    home_team_name,
                    team_code=match_info.get('home_team_code'),
                    logo_url=match_info.get('home_team_logo')
                )
                away_team_id = self.db.get_or_create_team(
                    away_team_name,
                    team_code=match_info.get('away_team_code'),
                    logo_url=match_info.get('away_team_logo')
                )
                
                if not home_team_id or not away_team_id:
                    logger.error(f"  {Fore.RED}[ERROR] Kh√¥ng th·ªÉ t·∫°o teams")
                    self.stats['matches_errors'] += 1
                    continue
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu match
                match_data = {
                    'home_team_id': home_team_id,
                    'away_team_id': away_team_id,
                    'home_team_name': home_team_name,
                    'away_team_name': away_team_name,
                    'match_date': match_date if isinstance(match_date, datetime) else datetime.now(),
                    'tournament_name': match_info.get('tournament_name', ''),
                    'category_id': match_info.get('category_id', 1),
                    'venue': match_info.get('venue', ''),
                    'status': match_info.get('status', 'scheduled')
                }
                
                # L∆∞u v√†o database
                match_id = self.db.insert_match(match_data)
                
                if match_id:
                    print(f"  {Fore.GREEN}[OK] ƒê√£ l∆∞u (ID: {match_id})")
                    self.stats['matches_saved'] += 1
                else:
                    print(f"  {Fore.YELLOW}[SKIP] B·ªè qua (ƒë√£ t·ªìn t·∫°i)")
                    self.stats['matches_skipped'] += 1
                
                # Delay gi·ªØa c√°c tr·∫≠n ƒë·∫•u
                time.sleep(1)
            
        except Exception as e:
            logger.error(f"[ERROR] L·ªói crawl matches t·ª´ {source_name}: {e}", exc_info=True)
            self.stats['matches_errors'] += 1
    
    def run(self, limit_per_source=50, days_range=None):
        """
        Ch·∫°y crawler cho t·∫•t c·∫£ c√°c ngu·ªìn l·ªãch thi ƒë·∫•u
        
        Args:
            limit_per_source: S·ªë l∆∞·ª£ng tr·∫≠n ƒë·∫•u t·ªëi ƒëa m·ªói ngu·ªìn
            days_range: Tuple (days_before, days_after) ƒë·ªÉ filter theo ng√†y
                       V√≠ d·ª•: (1, 1) = h√¥m qua, h√¥m nay, h√¥m sau
                       None = l·∫•y t·∫•t c·∫£ c√°c tr·∫≠n s·∫Øp di·ªÖn ra
        """
        self.print_header()
        
        start_time = time.time()
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl l·ªãch thi ƒë·∫•u l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if days_range:
            days_before, days_after = days_range
            now = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            start_date = now - timedelta(days=days_before)
            end_date = now + timedelta(days=days_after)
            print(f"{Fore.YELLOW}üìÖ L·ªçc tr·∫≠n ƒë·∫•u t·ª´ {start_date.strftime('%d/%m/%Y')} ƒë·∫øn {end_date.strftime('%d/%m/%Y')}{Style.RESET_ALL}\n")
        
        # Crawl c√°c tr·∫≠n ƒë·∫•u s·∫Øp di·ªÖn ra t·ª´ t·∫•t c·∫£ ngu·ªìn
        for source_name, source_config in MATCH_SOURCES.items():
            self.crawl_matches(source_name, source_config, limit=limit_per_source, days_range=days_range)
        
        # Th·ªëng k√™
        elapsed_time = time.time() - start_time
        self.print_stats()
        
        # Th·ªëng k√™ database
        db_stats = self.db.get_statistics()
        if db_stats:
            print(f"{Fore.CYAN}{'-'*70}")
            print(f"{Fore.CYAN}TH·ªêNG K√ä DATABASE:")
            
            if db_stats.get('total_matches'):
                print(f"{Fore.GREEN}  - T·ªïng s·ªë tr·∫≠n ƒë·∫•u: {db_stats['total_matches']}")
            
            if db_stats.get('matches_by_status'):
                print(f"{Fore.CYAN}  Tr·∫≠n ƒë·∫•u theo tr·∫°ng th√°i:")
                for status_info in db_stats['matches_by_status']:
                    print(f"{Fore.GREEN}    - {status_info['status']}: {status_info['count']}")
            
            print(f"{Fore.CYAN}{'-'*70}{Style.RESET_ALL}\n")
        
        logger.info(f"[OK] Ho√†n th√†nh trong {elapsed_time:.2f} gi√¢y")
        print(f"{Fore.GREEN}[OK] Crawler l·ªãch thi ƒë·∫•u ho√†n th√†nh!{Style.RESET_ALL}\n")
    
    def close(self):
        """ƒê√≥ng c√°c k·∫øt n·ªëi"""
        self.db.close()


def main():
    """H√†m main"""
    try:
        crawler = MatchCrawler()
        
        # Crawl l·ªãch thi ƒë·∫•u t·ª´ t·∫•t c·∫£ ngu·ªìn
        # days_range=(1, 1) = l·∫•y h√¥m qua, h√¥m nay, h√¥m sau
        crawler.run(limit_per_source=50, days_range=(1, 1))
        
        crawler.close()
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}[WARN] ƒê√£ d·ª´ng crawler b·ªüi ng∆∞·ªùi d√πng{Style.RESET_ALL}")
    except Exception as e:
        logger.error(f"[ERROR] L·ªói nghi√™m tr·ªçng: {e}", exc_info=True)


if __name__ == '__main__':
    main()

