# -*- coding: utf-8 -*-
"""
Base Parser - L·ªõp c∆° s·ªü cho c√°c parser
"""

import requests
from bs4 import BeautifulSoup
import logging
import time
from slugify import slugify
from config import USER_AGENT, REQUEST_TIMEOUT, RETRY_TIMES, DELAY_BETWEEN_REQUESTS
from datetime import datetime
from urllib.parse import urljoin

logger = logging.getLogger(__name__)


class BaseParser:
    """L·ªõp c∆° s·ªü cho t·∫•t c·∫£ c√°c parser"""
    
    def __init__(self, source_name, base_url):
        self.source_name = source_name
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': USER_AGENT})
    
    def get_page(self, url, retry=RETRY_TIMES):
        """L·∫•y n·ªôi dung trang web"""
        for attempt in range(retry):
            try:
                logger.info(f"üì° ƒêang t·∫£i: {url}")
                response = self.session.get(url, timeout=REQUEST_TIMEOUT)
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    time.sleep(DELAY_BETWEEN_REQUESTS)
                    return response.text
                else:
                    logger.warning(f"‚ö† HTTP {response.status_code}: {url}")
                    
            except Exception as e:
                logger.error(f"‚úó L·ªói t·∫£i trang (l·∫ßn {attempt + 1}/{retry}): {e}")
                if attempt < retry - 1:
                    time.sleep(5)
        
        return None
    
    def parse_soup(self, html):
        """Parse HTML th√†nh BeautifulSoup object"""
        return BeautifulSoup(html, 'lxml')
    
    def clean_text(self, text):
        """L√†m s·∫°ch text"""
        if not text:
            return ""
        return ' '.join(text.strip().split())
    
    def generate_slug(self, title):
        """T·∫°o slug t·ª´ ti√™u ƒë·ªÅ (unique v·ªõi timestamp)"""
        base_slug = slugify(title, separator='-')
        # Th√™m timestamp chi ti·∫øt ƒë·ªÉ ƒë·∫£m b·∫£o unique (gi·ªëng API PostController)
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        return f"{base_slug}-{timestamp}"
    
    def download_image(self, image_url, article_slug):
        """
        Tr·∫£ v·ªÅ URL ·∫£nh tr·ª±c ti·∫øp t·ª´ trang b√°o (kh√¥ng download)
        X·ª≠ l√Ω URL VnExpress CDN v√† validate format
        """
        try:
            # T·∫°o absolute URL n·∫øu l√† relative path
            if not image_url.startswith('http'):
                image_url = urljoin(self.base_url, image_url)
            
            # Validate URL
            if not image_url.startswith('http'):
                logger.warning(f"  ‚ö† URL ·∫£nh kh√¥ng h·ª£p l·ªá: {image_url}")
                return None
            
            # X·ª≠ l√Ω URL VnExpress CDN - ƒë·∫£m b·∫£o URL ƒë·∫ßy ƒë·ªß
            # VnExpress CDN URLs th∆∞·ªùng c√≥ format: https://i1-thethao.vnecdn.net/YYYY/MM/DD/filename-timestamp
            # Gi·ªØ nguy√™n query parameters v√¨ ch√∫ng c√≥ th·ªÉ ch·ª©a th√¥ng tin resize/optimize
            if 'vnecdn.net' in image_url:
                # Gi·ªØ nguy√™n query parameters (kh√¥ng x√≥a)
                # N·∫øu URL kh√¥ng c√≥ extension trong filename, log ƒë·ªÉ debug
                filename = image_url.split('?')[0].split('/')[-1]
                if '.' not in filename:
                    logger.debug(f"  ‚Ñπ URL CDN kh√¥ng c√≥ extension r√µ r√†ng: {image_url[:60]}...")
            
            # Validate URL c√≥ v·∫ª h·ª£p l·ªá
            logger.info(f"  ‚úì S·ª≠ d·ª•ng URL ·∫£nh g·ªëc: {image_url[:60]}...")
            return image_url
                
        except Exception as e:
            logger.error(f"‚úó L·ªói x·ª≠ l√Ω URL ·∫£nh {image_url}: {e}")
        
        return None
    
    def process_content_images(self, content_html, article_slug):
        """
        X·ª≠ l√Ω t·∫•t c·∫£ ·∫£nh trong content HTML:
        - T√¨m t·∫•t c·∫£ th·∫ª <img>
        - Chuy·ªÉn ƒë·ªïi th√†nh URL tuy·ªát ƒë·ªëi (kh√¥ng download)
        - Thay th·∫ø src c≈© b·∫±ng URL g·ªëc
        
        Args:
            content_html: BeautifulSoup object ho·∫∑c string HTML
            article_slug: Slug c·ªßa b√†i vi·∫øt (ƒë·ªÉ ƒë·∫∑t t√™n file)
            
        Returns:
            HTML string ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (v·ªõi URL ·∫£nh tuy·ªát ƒë·ªëi)
        """
        try:
            # Parse HTML n·∫øu l√† string
            if isinstance(content_html, str):
                soup = BeautifulSoup(content_html, 'lxml')
            else:
                soup = content_html
            
            # T√¨m t·∫•t c·∫£ th·∫ª img
            img_tags = soup.find_all('img')
            
            if not img_tags:
                logger.info("‚Ñπ Kh√¥ng c√≥ ·∫£nh trong content")
                return str(soup)
            
            logger.info(f"üñºÔ∏è T√¨m th·∫•y {len(img_tags)} ·∫£nh trong content, ƒëang x·ª≠ l√Ω URL...")
            
            processed_count = 0
            for img in img_tags:
                # L·∫•y URL ·∫£nh (th·ª≠ nhi·ªÅu thu·ªôc t√≠nh)
                img_url = img.get('data-src') or img.get('src') or img.get('data-original')
                
                if not img_url:
                    continue
                
                # Chuy·ªÉn th√†nh URL tuy·ªát ƒë·ªëi
                absolute_url = self.download_image(img_url, article_slug)
                
                if absolute_url:
                    # Thay th·∫ø URL c≈© b·∫±ng URL g·ªëc
                    img['src'] = absolute_url
                    
                    # X√≥a c√°c thu·ªôc t√≠nh lazy load
                    if img.get('data-src'):
                        del img['data-src']
                    if img.get('data-original'):
                        del img['data-original']
                    
                    processed_count += 1
                else:
                    logger.warning(f"  ‚ö† Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c: {img_url[:50]}...")
            
            logger.info(f"‚úì ƒê√£ x·ª≠ l√Ω {processed_count}/{len(img_tags)} ·∫£nh trong content")
            
            # Tr·∫£ v·ªÅ HTML ƒë√£ x·ª≠ l√Ω
            # L·∫•y body content (b·ªè c√°c th·∫ª html, body t·ª± ƒë·ªông th√™m v√†o)
            body = soup.find('body')
            if body:
                return ''.join(str(child) for child in body.children)
            else:
                return str(soup)
            
        except Exception as e:
            logger.error(f"‚úó L·ªói x·ª≠ l√Ω ·∫£nh trong content: {e}")
            return str(content_html)
    
    def detect_category(self, title, content, url):
        """Ph√°t hi·ªán category t·ª´ n·ªôi dung (override trong subclass)"""
        from config import CATEGORY_MAPPING
        
        text = f"{title} {content} {url}".lower()
        
        for keyword, category_id in CATEGORY_MAPPING.items():
            if keyword in text:
                return category_id
        
        return 1  # Default: B√≥ng ƒë√°
    
    def extract_tags(self, title, content):
        """Tr√≠ch xu·∫•t tags t·ª´ n·ªôi dung"""
        tags = []
        
        # Danh s√°ch c√°c keywords ph·ªï bi·∫øn trong b√≥ng ƒë√°
        common_keywords = [
            'Premier League', 'La Liga', 'Serie A', 'Bundesliga',
            'Champions League', 'Europa League', 'World Cup',
            'Manchester United', 'Liverpool', 'Real Madrid', 'Barcelona',
            'Arsenal', 'Chelsea', 'Man City', 'PSG', 'Bayern Munich',
            'Messi', 'Ronaldo', 'Neymar', 'Mbappe',
            'V-League', 'AFF Cup', 'SEA Games',
            'Chuy·ªÉn nh∆∞·ª£ng', 'Transfer', 'HLV', 'Coach'
        ]
        
        text = f"{title} {content}"
        
        for keyword in common_keywords:
            if keyword.lower() in text.lower():
                tags.append(keyword)
        
        return tags[:10]  # Gi·ªõi h·∫°n 10 tags
    
    def get_article_list(self):
        """L·∫•y danh s√°ch b√†i vi·∫øt (ph·∫£i override trong subclass)"""
        raise NotImplementedError("Ph·∫£i implement method get_article_list()")
    
    def parse_article(self, url):
        """Parse chi ti·∫øt m·ªôt b√†i vi·∫øt (ph·∫£i override trong subclass)"""
        raise NotImplementedError("Ph·∫£i implement method parse_article()")

