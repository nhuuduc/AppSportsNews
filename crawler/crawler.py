# -*- coding: utf-8 -*-
"""
Main Crawler - Crawler ch√≠nh cho h·ªá th·ªëng
"""

import logging
from logging.handlers import RotatingFileHandler
from colorama import init, Fore, Style
from database import DatabaseHandler
from parsers import VnExpressParser
from config import NEWS_SOURCES, LOG_FILE
import time
from datetime import datetime

# Initialize colorama
init(autoreset=True)

# Setup logging
def setup_logging():
    """C·∫•u h√¨nh logging"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # File handler
    file_handler = RotatingFileHandler(
        LOG_FILE, 
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

logger = setup_logging()


class NewsCrawler:
    """Crawler ch√≠nh cho tin t·ª©c th·ªÉ thao"""
    
    def __init__(self):
        self.db = DatabaseHandler()
        self.parsers = {
            'VnExpressParser': VnExpressParser(),
        }
        self.stats = {
            'total_crawled': 0,
            'total_saved': 0,
            'total_skipped': 0,
            'total_errors': 0
        }
    
    def print_header(self):
        """In header ƒë·∫πp"""
        print(f"\n{Fore.CYAN}{'='*70}")
        print(f"{Fore.CYAN}‚ïë{' '*68}‚ïë")
        print(f"{Fore.CYAN}‚ïë{Fore.YELLOW}{'SPORTS NEWS CRAWLER':^68}{Fore.CYAN}‚ïë")
        print(f"{Fore.CYAN}‚ïë{Fore.GREEN}{'C√¥ng c·ª• crawl tin t·ª©c th·ªÉ thao t·ª± ƒë·ªông':^68}{Fore.CYAN}‚ïë")
        print(f"{Fore.CYAN}‚ïë{' '*68}‚ïë")
        print(f"{Fore.CYAN}{'='*70}{Style.RESET_ALL}\n")
    
    def print_stats(self):
        """In th·ªëng k√™"""
        print(f"\n{Fore.YELLOW}{'‚îÄ'*70}")
        print(f"{Fore.YELLOW}TH·ªêNG K√ä CRAWLER:")
        print(f"{Fore.GREEN}  ‚úì T·ªïng s·ªë b√†i crawl: {self.stats['total_crawled']}")
        print(f"{Fore.GREEN}  ‚úì ƒê√£ l∆∞u th√†nh c√¥ng: {self.stats['total_saved']}")
        print(f"{Fore.YELLOW}  ‚ö† ƒê√£ b·ªè qua (tr√πng): {self.stats['total_skipped']}")
        print(f"{Fore.RED}  ‚úó L·ªói: {self.stats['total_errors']}")
        print(f"{Fore.YELLOW}{'‚îÄ'*70}{Style.RESET_ALL}\n")
    
    def crawl_source(self, source_name, source_config, limit=10):
        """Crawl m·ªôt ngu·ªìn tin"""
        if not source_config.get('enabled', False):
            logger.info(f"‚äó Ngu·ªìn {source_name} ƒë√£ b·ªã t·∫Øt")
            return
        
        print(f"\n{Fore.CYAN}‚ñ∂ B·∫Øt ƒë·∫ßu crawl: {source_config['name']}")
        print(f"{Fore.CYAN}  URL: {source_config['base_url']}{Style.RESET_ALL}")
        
        parser_name = source_config.get('parser')
        if parser_name not in self.parsers:
            logger.error(f"‚úó Kh√¥ng t√¨m th·∫•y parser: {parser_name}")
            return
        
        parser = self.parsers[parser_name]
        
        try:
            # L·∫•y danh s√°ch b√†i vi·∫øt
            articles = parser.get_article_list(limit=limit)
            
            if not articles:
                logger.warning(f"‚ö† Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o t·ª´ {source_name}")
                return
            
            print(f"{Fore.GREEN}  ‚úì T√¨m th·∫•y {len(articles)} b√†i vi·∫øt\n")
            
            # Parse v√† l∆∞u t·ª´ng b√†i vi·∫øt
            for idx, article_info in enumerate(articles, 1):
                print(f"{Fore.CYAN}  [{idx}/{len(articles)}] {article_info['title'][:60]}...")
                
                self.stats['total_crawled'] += 1
                
                # Parse chi ti·∫øt b√†i vi·∫øt
                article_data = parser.parse_article(article_info['url'])
                
                if not article_data:
                    logger.error(f"  {Fore.RED}‚úó Kh√¥ng th·ªÉ parse b√†i vi·∫øt")
                    self.stats['total_errors'] += 1
                    continue
                
                # L∆∞u v√†o database
                article_id = self.db.insert_article(article_data)
                
                if article_id:
                    print(f"  {Fore.GREEN}‚úì ƒê√£ l∆∞u (ID: {article_id})")
                    self.stats['total_saved'] += 1
                    
                    # Th√™m tags
                    if article_data.get('tags'):
                        self.db.insert_article_tags(article_id, article_data['tags'])
                    
                    # Th√™m images
                    if article_data.get('images'):
                        self.db.insert_article_images(article_id, article_data['images'])
                else:
                    print(f"  {Fore.YELLOW}‚ö† B·ªè qua (ƒë√£ t·ªìn t·∫°i)")
                    self.stats['total_skipped'] += 1
                
                # Delay gi·ªØa c√°c b√†i vi·∫øt
                time.sleep(2)
            
        except Exception as e:
            logger.error(f"‚úó L·ªói crawl ngu·ªìn {source_name}: {e}")
            self.stats['total_errors'] += 1
    
    def run(self, limit_per_source=10):
        """Ch·∫°y crawler cho t·∫•t c·∫£ c√°c ngu·ªìn"""
        self.print_header()
        
        start_time = time.time()
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu crawl l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Crawl t·ª´ng ngu·ªìn tin t·ª©c
        for source_name, source_config in NEWS_SOURCES.items():
            self.crawl_source(source_name, source_config, limit=limit_per_source)
        
        # Th·ªëng k√™
        elapsed_time = time.time() - start_time
        self.print_stats()
        
        # Th·ªëng k√™ database
        db_stats = self.db.get_statistics()
        if db_stats:
            print(f"{Fore.CYAN}{'‚îÄ'*70}")
            print(f"{Fore.CYAN}TH·ªêNG K√ä DATABASE:")
            print(f"{Fore.GREEN}  ‚Ä¢ T·ªïng s·ªë b√†i vi·∫øt: {db_stats['total_articles']}")
            print(f"{Fore.GREEN}  ‚Ä¢ T·ªïng s·ªë categories: {db_stats['total_categories']}")
            print(f"{Fore.GREEN}  ‚Ä¢ T·ªïng s·ªë tags: {db_stats['total_tags']}")
            
            if db_stats.get('by_status'):
                print(f"{Fore.CYAN}  B√†i vi·∫øt theo tr·∫°ng th√°i:")
                for status_info in db_stats['by_status']:
                    print(f"{Fore.GREEN}    - {status_info['status']}: {status_info['count']}")
            
            print(f"{Fore.CYAN}{'‚îÄ'*70}{Style.RESET_ALL}\n")
        
        logger.info(f"‚úì Ho√†n th√†nh trong {elapsed_time:.2f} gi√¢y")
        print(f"{Fore.GREEN}‚úì Crawler ho√†n th√†nh!{Style.RESET_ALL}\n")
    
    def close(self):
        """ƒê√≥ng c√°c k·∫øt n·ªëi"""
        self.db.close()


def main():
    """H√†m main"""
    try:
        crawler = NewsCrawler()
        
        # Crawl 10 b√†i vi·∫øt t·ª´ m·ªói ngu·ªìn
        crawler.run(limit_per_source=10)
        
        crawler.close()
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}‚ö† ƒê√£ d·ª´ng crawler b·ªüi ng∆∞·ªùi d√πng{Style.RESET_ALL}")
    except Exception as e:
        logger.error(f"‚úó L·ªói nghi√™m tr·ªçng: {e}", exc_info=True)


if __name__ == '__main__':
    main()

